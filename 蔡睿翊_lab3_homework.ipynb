{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name: 蔡睿翊\n",
    "\n",
    "Student ID: 112065802\n",
    "\n",
    "GitHub ID: [vincenttsai2015](https://github.com/vincenttsai2015/)\n",
    "\n",
    "Kaggle name: juiyitsai\n",
    "\n",
    "Kaggle private scoreboard snapshot: \n",
    "\n",
    "[Snapshot](蔡睿翊_rank.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 40% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/t/6132d98db776a0745496d3ebbe011f3c) regarding Emotion Recognition on Twitter by this link https://www.kaggle.com/t/6132d98db776a0745496d3ebbe011f3c. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 30% of the 40% available for this section.\n",
    "\n",
    "    - **Top 41% - 100%**: Get (60-x)/6 + 30 points, where x is your ranking in the leaderboard (ie. If you rank 3rd your score will be (60-3)/6 + 20 = 39.5% out of 40%)   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 19th 11:59 pm, Tuesday)__. Make sure to take a screenshot of your position at the end of the competition.\n",
    "    \n",
    "\n",
    "2. Second: __This part is worth 40% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "3. Third: __This part is worth 20% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "Upload your files to your repository then submit the assignment with the regulated format to the [folder](https://drive.google.com/drive/folders/1auARVdUHtww5U_T6MDeiZ8ApZ_ANjeYl).\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 20th 11:59 pm, Wednesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the necessary tools\n",
    "First, we need to import the necessary tools including general ones and those from Pytorch and huggingface as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Begin Assignment Here\n",
    "# import general tools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import DL tools\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "#### Raw data parsing\n",
    "From the given dataset, we load the csv files of the ground truth and identification (identifying training/testing instances) by Pandas to create two dataframes ```df_groundtruth``` and ```df_identification```, respectively. And we load the IDs and texts raw data line by line into two respective lists, which are then used to build a dataframe ```df_raw```. Then we merge ```df_identification``` and ```df_raw``` on the column ```tweet_id``` to create a merged dataframe ```df_raw_merge```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Raw data parsing...')\n",
    "# groundtruth\n",
    "df_groundtruth = pd.read_csv('emotion.csv')\n",
    "\n",
    "# train-test-splitting\n",
    "df_identification = pd.read_csv('data_identification.csv')\n",
    "\n",
    "# raw data parsing\n",
    "tweet_ids = []\n",
    "textual_data = []\n",
    "with open(\"tweets_DM.json\",\"r\") as jsfile:\n",
    "    for line in jsfile.readlines():\n",
    "        dic = json.loads(line)\n",
    "        tweet_ids.append(dic['_source']['tweet']['tweet_id'])\n",
    "        textual_data.append(dic['_source']['tweet']['text'])\n",
    "\n",
    "# merging with identification for splitting train/test data\n",
    "df_raw = pd.DataFrame({'tweet_id': tweet_ids, 'text': textual_data})\n",
    "df_raw_merge = pd.merge(df_raw, df_identification, on='tweet_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tidying up training/testing data\n",
    "We split ```df_raw_merge``` into training and testing parts according to column ```identification```. For the testing part, we load the template \"sampleSubmission.csv\" to create dataframe ```df_submit_samples```, drop column ```emotion``` to create dataframe ```df_test_ID``` and rename column ```id``` by ```tweet_id``` to align with the testing part when merging ```df_test_ID``` and ```df_raw_merge_test``` (the testing parts of ```df_raw_merge```). \n",
    "\n",
    "Dropping columns ```tweet_id``` and ```identification```, we have clean training and testing dataframes ```df_train``` and ```df_test```.\n",
    "\n",
    "To perform validation during the training process, we further divide ```df_train``` into the training part and validation part by sampling 20% data of ```df_train``` as validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tidying up training data...')\n",
    "# preparing training data and labels\n",
    "df_identification_train = df_identification[df_identification['identification']=='train']\n",
    "df_raw_merge_train = df_raw_merge[df_raw_merge['identification']=='train']\n",
    "df_raw_merge_train = pd.merge(df_raw_merge_train, df_groundtruth, on='tweet_id', how='left')\n",
    "\n",
    "print('Tidying up testing data...')\n",
    "# preparing testing data \n",
    "df_submit_samples = pd.read_csv(\"sampleSubmission.csv\",encoding=\"utf-8\")\n",
    "df_submit_samples.rename(columns = {'id':'tweet_id'}, inplace = True)\n",
    "df_test_ID = df_submit_samples.drop(columns=['emotion'])\n",
    "df_identification_test = df_identification[df_identification['identification']=='test']\n",
    "df_raw_merge_test = df_raw_merge[df_raw_merge['identification']=='test']\n",
    "df_raw_merge_test = pd.merge(df_test_ID, df_raw_merge_test, on='tweet_id', how='left')\n",
    "\n",
    "df_train = df_raw_merge_train.drop(columns=['tweet_id', 'identification'])\n",
    "df_test = df_raw_merge_test.drop(columns=['tweet_id', 'identification'])\n",
    "\n",
    "print('Sampling validation data from training data...')\n",
    "# split the training data into training part and validation part\n",
    "df_val = df_train.sample(frac=0.2, random_state=30)\n",
    "df_train = df_train.drop(df_val.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 8 classes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available labels and tokenizer\n",
    "available_labels = df_raw_merge_train['emotion'].unique().tolist()\n",
    "all_labels = {element: count for count, element in enumerate(available_labels)}\n",
    "# {'anticipation':0, 'sadness':1, 'fear':2, 'joy':3, 'anger':4, 'trust':5, 'disgust':6, 'surprise':7}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset module\n",
    "* To construct dataloaders for the learning process, we implement a dataset module ```KaggleSentimentDataset``` that inherits ```torch.utils.data.Dataset```. \n",
    "* In this module, we collect texts encoded and tokenized by ```tokenizer``` that calls ```BertTokenizer``` in module ```transformer``` and remove noises such as unnecessary stopwords and punctuations, etc., with modules such as nltk (Natural Language Toolkit) and re (regular expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tokenizing...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')  \n",
    "\n",
    "# self-defined dataset module\n",
    "class KaggleSentimentDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        texts = df.text.values.tolist()\n",
    "        texts = [self._preprocess(text) for text in texts]\n",
    "        self.texts = [tokenizer(text, padding='max_length', max_length=128, truncation=True, return_tensors=\"pt\") for text in texts]\n",
    "        if 'emotion' in df:\n",
    "            self.labels = [all_labels[label] for label in df['emotion']]\n",
    "        else:\n",
    "            self.labels = [-1] * len(df)\n",
    "    \n",
    "    def _preprocess(self, text):\n",
    "        text = self._remove_amp(text)\n",
    "        text = self._remove_links(text)\n",
    "        text = self._remove_hashes(text)\n",
    "        text = self._remove_retweets(text)\n",
    "        text = self._remove_mentions(text)\n",
    "        text = self._remove_multiple_spaces(text)\n",
    "        text = self._remove_punctuation(text)\n",
    "\n",
    "        text_tokens = self._tokenize(text)\n",
    "        text_tokens = self._stopword_filtering(text_tokens)\n",
    "        text = self._stitch_text_tokens_together(text_tokens)\n",
    "\n",
    "        return text.strip()\n",
    "\n",
    "    def _remove_amp(self, text):\n",
    "        return text.replace(\"&amp;\", \" \")\n",
    "    \n",
    "    def _remove_links(self, text):\n",
    "        return re.sub(r'https?:\\/\\/[^\\s\\n\\r]+', ' ', text)\n",
    "\n",
    "    def _remove_hashes(self, text):\n",
    "        return re.sub(r'#', ' ', text)\n",
    "    \n",
    "    def _remove_retweets(self, text):\n",
    "        return re.sub(r'^RT[\\s]+', ' ', text)\n",
    "    \n",
    "    def _remove_mentions(self, text):\n",
    "        return re.sub(r'(@.*?)[\\s]', ' ', text)\n",
    "    \n",
    "    def _remove_multiple_spaces(self, text):\n",
    "        return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return ''.join(character for character in text if character not in string.punctuation)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return nltk.word_tokenize(text, language=\"english\")\n",
    "\n",
    "    def _stopword_filtering(self, text_tokens):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        return [token for token in text_tokens if token not in stop_words]\n",
    "\n",
    "    def _stitch_text_tokens_together(self, text_tokens):\n",
    "        return \" \".join(text_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = -1\n",
    "        if hasattr(self, 'labels'):\n",
    "            label = self.labels[idx]\n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataloader construction\n",
    "With the implementation of module ```KaggleSentimentDataset```, we pack the training/validation/test dataframes into training/validation/testing datasets and load them into respective dataloaders with ```batch_size=16```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset construction...\n",
      "Validation dataset construction...\n",
      "Testing dataset construction...\n"
     ]
    }
   ],
   "source": [
    "# construct the datasets\n",
    "print('Training dataset construction...')\n",
    "training_dataset = KaggleSentimentDataset(df_train, tokenizer)\n",
    "print('Validation dataset construction...')\n",
    "validation_dataset = KaggleSentimentDataset(df_val, tokenizer)\n",
    "print('Testing dataset construction...')\n",
    "testing_dataset = KaggleSentimentDataset(df_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders\n",
    "print('Building dataloaders...')\n",
    "train_dataloader = DataLoader(training_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "val_dataloader = DataLoader(validation_dataset, batch_size=16, num_workers=0)\n",
    "test_dataloader = DataLoader(testing_dataset, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Classification Model\n",
    "We build a sentiment classification model with the implementation of class ```BertSentimentClassifier``` as follows.\n",
    "\n",
    "#### Base model\n",
    "We adopt pretrained RoBERTa (a variation of BERT) as a base model by importing ```BertModel``` in ```transformers``` with specification of model name ```roberta-base```.\n",
    "\n",
    "#### Fine-tune the base model\n",
    "We fine-tune the base model by adding 2 fully connected layers ```fc1``` and ```fc2``` and a ReLU layer ```relu``` to derive the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing BERT model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34d3675682649eea26075b35987a1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f83db55f40ef4ff2a3f48450743c676d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['encoder.layer.1.attention.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.2.attention.self.query.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.2.output.dense.weight', 'embeddings.word_embeddings.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.10.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.output.dense.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.bias', 'pooler.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "class BertSentimentClassifier(Module):\n",
    "    def __init__(self, base_model, dropout=0.5):\n",
    "        super(BertSentimentClassifier, self).__init__()\n",
    "        self.bert = base_model\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(768, 32)\n",
    "        self.fc2 = nn.Linear(32, 8) # len(all_labels)=8\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids=input_id, attention_mask=mask, return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output_1 = self.fc1(dropout_output)\n",
    "        linear_output_2 = self.fc2(linear_output_1)\n",
    "        final_layer = self.relu(linear_output_2)\n",
    "        return final_layer\n",
    "\n",
    "# model initialization\n",
    "print('Initializing BERT model...')\n",
    "base_model = BertModel.from_pretrained(\"roberta-base\")\n",
    "model = BertSentimentClassifier(base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation\n",
    "After initializing the sentiment classification model, we define cross-entropy loss with ```CrossEntropyLoss()``` and load training and validation dataloaders, we start the training and validation processes as follows.\n",
    "\n",
    "#### Training\n",
    "* We train the fine-tuned model with training dataloader in 3 epochs with learning rate ```0.00002 (2e-5)``` and optimizer ```Adam```.\n",
    "* The ```output``` is derived by inputting ```train_input['attention_mask']``` and ```train_input['input_ids']```.\n",
    "* Use ```output``` and ```train_label``` to calculate the loss (averaged by the length of training dataloader) and accuracy (averaged by the size of training instances). \n",
    "\n",
    "#### Validation\n",
    "* In each epoch, we validate the trained model with validation dataloader.\n",
    "* The process is basically the same with the training process (except for model update).\n",
    "* To avoid overfitting, we keep monitoring the best validation loss in each epoch and set the count of early stopping to 3.\n",
    "* If the current validation loss is lower than ```best_val_loss```, we update the current validation loss to ```best_val_loss``` and save the trained model as ```best_model.pt``` for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "def train(model, train_ldr, val_ldr, learning_rate, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_threshold_count = 0\n",
    "    \n",
    "    # GPU usage determination\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    # Defining loss function and optimizer\n",
    "    criterion = CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Send the model to GPU memory\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "    # training iteration\n",
    "    for epoch_num in range(epochs):\n",
    "        print(f'Epoch: {epoch_num}')\n",
    "        # training accuracy and loss\n",
    "        train_acc = []\n",
    "        train_loss = 0\n",
    "        \n",
    "        model.train()\n",
    "        # tqdm\n",
    "        for train_input, train_label in tqdm(train_ldr):\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # prediction result\n",
    "            output = model(input_id, mask)\n",
    "            # loss\n",
    "            batch_loss = criterion(output, train_label)\n",
    "            train_loss += batch_loss.item()\n",
    "            \n",
    "            # model update\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # accuracy\n",
    "            output_index = output.argmax(axis=1)\n",
    "            acc = (output_index == train_label)\n",
    "            train_acc += acc\n",
    "\n",
    "        train_accuracy = (sum(train_acc)/len(train_acc)).item()\n",
    "        print(f'Train Loss: {train_loss / len(train_ldr): .3f} | Train Accuracy: {train_accuracy: 10.3%}')\n",
    "        \n",
    "        # Model validation\n",
    "        model.eval()\n",
    "        # validation accuracy and loss\n",
    "        val_acc = []\n",
    "        val_loss = 0        \n",
    "        with torch.no_grad(): # no need to compute gradient\n",
    "            # validate with trained model\n",
    "            for val_input, val_label in tqdm(val_ldr):\n",
    "                # same process with training\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input['attention_mask'].to(device)\n",
    "                input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "                # prediction result\n",
    "                output = model(input_id, mask)\n",
    "                # loss\n",
    "                batch_loss = criterion(output, val_label)\n",
    "                val_loss += batch_loss.item()\n",
    "                \n",
    "                # accuracy\n",
    "                output_index = output.argmax(axis=1)\n",
    "                acc = (output_index == val_label)\n",
    "                val_acc += acc\n",
    "            val_accuracy = (sum(val_acc)/len(val_acc)).item()            \n",
    "            print(f'Val Loss: {val_loss / len(val_ldr): .3f} | Val Accuracy: {val_accuracy: 10.3%}')\n",
    "            \n",
    "            if best_val_loss > val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                torch.save(model, f\"best_model.pt\")\n",
    "                print(\"Saved model\")\n",
    "                early_stopping_threshold_count = 0\n",
    "            else:\n",
    "                early_stopping_threshold_count += 1\n",
    "            if early_stopping_threshold_count >= 3:\n",
    "                print(\"Early stopping\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training...')\n",
    "EPOCHS = 3\n",
    "LR = 2e-5\n",
    "train(model, train_dataloader, val_dataloader, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the screenshot of the training procedure, \n",
    "* With batch size = 16, it takes around 2 hours for training one epoch on an NVIDIA RTX 3090 GPU card.\n",
    "* If the training epoch number is set to be large (e.g., 10), the training accuracy can reach 72.847% at epoch 9. \n",
    "* However, at epoch 3, the validation error starts to increase, which demonstrates that the model starts to overfit. \n",
    "* Thus the ideal number of training epochs is 3.\n",
    "\n",
    "![training_process.png](training_process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "* We use the tesing dataloader and the model saved in validation process to conduct testing. \n",
    "* The prediction results in ```pred_results``` consist of the IDs of the sentiment classes.\n",
    "* To meet the format requirement for uploading the prediction result to Kaggle\n",
    "    * update the column ```emotion``` of the dataframe ```sample_submission``` that loads the file \"sampleSubmission.csv\" by ```pred_results```.\n",
    "    * convert the prediction result to the corresponding sentiment class name through mapping with the dictionary ```id2label```.    \n",
    "* Save the updated dataframe as a csv file (e.g., \"final_submission_112065802.csv\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_predictions(model, test_ldr):\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = model.to(device)    \n",
    "    \n",
    "    results_predictions = []\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        for data_input, _ in tqdm(test_ldr):\n",
    "            attention_mask = data_input['attention_mask'].to(device)\n",
    "            input_ids = data_input['input_ids'].squeeze(1).to(device)\n",
    "            output = model(input_ids, attention_mask)            \n",
    "            output_index = output.argmax(axis=1)\n",
    "            results_predictions.append(output_index)\n",
    "    \n",
    "    return torch.cat(results_predictions).cpu().detach().numpy()\n",
    "\n",
    "print('Testing...')\n",
    "pred_model = torch.load(\"best_model.pt\")\n",
    "sample_submission = pd.read_csv(\"sampleSubmission.csv\")\n",
    "pred_results = get_text_predictions(pred_model, test_dataloader)\n",
    "id2label = {count: element for count, element in enumerate(available_labels)}\n",
    "sample_submission[\"emotion\"] = pred_results\n",
    "sample_submission[\"emotion\"] = sample_submission[\"emotion\"].map(id2label)\n",
    "sample_submission.to_csv(\"final_submission_112065802.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postscript\n",
    "* In the beginning, since I did not totally get what to do, I used TA's codes in [this link](https://github.com/KevinCodePlace/NTHU_Data_Mining_2022Fall/blob/main/DM2022-Lab2/DM2022-Lab2-Homework-111065542.ipynb) to go through the whole process of this homework assignment and figure out how to imitate the implementation. After going through the whole process with TA's codes, I got 55%, the highest at that time. \n",
    "\n",
    "* But after trying to implement the modules of dataset construction, model building and training procedures on my own (with references found on the Internet), the resulting performance failed to be higher than 50% (even in the last submission......) by the deadline. \n",
    "\n",
    "* Thus I chose the best performance derived by my own implementation (which is 46.757%) in Kaggle instead of the initial one (that achieves 55% by using TA's codes)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
